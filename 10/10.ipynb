{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5093e2",
   "metadata": {},
   "source": [
    "９０．次単語予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a4793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トークン列: ['The', 'Ġmovie', 'Ġwas', 'Ġfull', 'Ġof']\n",
      "Ġjokes: 0.0219\n",
      "Ġgreat: 0.0186\n",
      "Ġlaughs: 0.0115\n",
      "Ġbad: 0.0109\n",
      "Ġsurprises: 0.0107\n",
      "Ġreferences: 0.0105\n",
      "Ġfun: 0.0100\n",
      "Ġhumor: 0.0074\n",
      "Ġ\": 0.0074\n",
      "Ġthe: 0.0067\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"The movie was full of\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"トークン列:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    next_token_logits = outputs.logits[0, -1, :]\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "    top_k = torch.topk(probs, k=10)\n",
    "\n",
    "top_tokens = tokenizer.convert_ids_to_tokens(top_k.indices.tolist())\n",
    "top_probs = top_k.values.tolist()\n",
    "\n",
    "for token, prob in zip(top_tokens, top_probs):\n",
    "    print(f\"{token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e55fc",
   "metadata": {},
   "source": [
    "９１．続きのテキスト予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f7b2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature: 0.7 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was full of funny references that people were talking about. The joke I got was that he was like, \"You've got to get out\n",
      "\n",
      "--- Temperature: 1.0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was full of jokes but the most telling aspect of this particular show was the way that it kept its emotional edge. \"I love you,\n",
      "\n",
      "--- Temperature: 1.5 ---\n",
      "The movie was full of good things. And I'd rather have the bad stuff. (Laughs.)\n",
      "\n",
      "Did anyone watch the film without reading about\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"The movie was full of\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# 温度パラメータや生成方式を変えてみる\n",
    "for temp in [0.7, 1.0, 1.5]:\n",
    "    print(f\"\\n--- Temperature: {temp} ---\")\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=30,\n",
    "        do_sample=True,        # sampling を有効にする\n",
    "        temperature=temp,\n",
    "        top_k=50,              # トップK制御（任意）\n",
    "        top_p=0.95,            # nucleus sampling（任意）\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717d748",
   "metadata": {},
   "source": [
    "９２．予測されたテキストの確率を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb18a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fun: 0.0100\n",
      " moments: 0.0549\n",
      ",: 0.3279\n",
      " and: 0.1136\n",
      " no: 0.0023\n",
      " one: 0.3667\n",
      " was: 0.1205\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The movie was full of\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=12, do_sample=True)\n",
    "generated_ids = output[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(generated_ids.unsqueeze(0))\n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "for i in range(len(input_ids[0]), len(generated_ids)):\n",
    "    token_id = generated_ids[i].item()\n",
    "    prob = probs[0, i - 1, token_id].item()\n",
    "    token = tokenizer.decode([token_id])\n",
    "    print(f\"{token}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9667d3",
   "metadata": {},
   "source": [
    "９３．パープレキシティ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de2c1e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was full of surprises -> Perplexity: 99.35\n",
      "The movies were full of surprises -> Perplexity: 126.48\n",
      "The movie were full of surprises -> Perplexity: 278.88\n",
      "The movies was full of surprises -> Perplexity: 274.66\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "def perplexity(sentence):\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "    return exp(loss.item())\n",
    "\n",
    "sentences = [\n",
    "    \"The movie was full of surprises\",\n",
    "    \"The movies were full of surprises\",\n",
    "    \"The movie were full of surprises\",   # 文法誤り\n",
    "    \"The movies was full of surprises\"    # 文法誤り\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    ppl = perplexity(s)\n",
    "    print(f\"{s} -> Perplexity: {ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5e707",
   "metadata": {},
   "source": [
    "９４，チャットテンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf537bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:\n",
      "What do you call a sweet eaten after dinner?\n",
      "### Assistant:\n",
      "### Human:\n",
      "what does it say it says?\n",
      "## Human:\n",
      "what does it say it says about you?\n",
      "Human:\n",
      "How are\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"### Human:\\n\"\n",
    "    \"What do you call a sweet eaten after dinner?\\n\"\n",
    "    \"### Assistant:\\n\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50, do_sample=True, temperature=0.9)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a057510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
